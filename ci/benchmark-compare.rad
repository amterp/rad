#!/usr/bin/env rad
---
Compares benchmark performance between base branch and current PR.
Uses interleaved hyperfine execution for reliable statistical comparison.
Outputs structured data for GitHub Actions consumption.
---

// Get base branch from GitHub Actions environment
base_ref = get_env("GITHUB_BASE_REF")
if not base_ref:
    base_ref = "main"
    print("âš ï¸ No GITHUB_BASE_REF found, using 'main' as base")

print("ðŸƒ Starting benchmark comparison...")
print("Base branch: {base_ref}")

// Get absolute path for PR directory
_, pwd_output, _ = $`pwd`
pr_dir = trim(pwd_output)
print("ðŸ”§ Working directory: {pr_dir}")

// Install hyperfine if not available (CI only - assumes hyperfine available locally)
ci_env = get_env("CI")
if ci_env == "true":
    print("ðŸ”§ Installing hyperfine in CI environment...")
    $`sudo apt-get update -qq && sudo apt-get install -y hyperfine`
    print("âœ… hyperfine installed")
else:
    print("ðŸ”§ Using local hyperfine installation")

// Define available benchmarks with fallback locations for backward compatibility
benchmark_configs = [
    {
        "new_path": "ci/benchmark-scripts/for-loop-add.rad",
        "old_path": "benchmark/scripts/for-loop-add.rad",
        "name": "for-loop-add"
    },
    {
        "new_path": "ci/benchmark-scripts/startup-time.rad",
        "old_path": null,
        "name": "startup-time"
    },
    {
        "new_path": "ci/benchmark-scripts/shell-commands.rad",
        "old_path": null,
        "name": "shell-commands"
    },
    {
        "new_path": "ci/benchmark-scripts/string-operations.rad",
        "old_path": null,
        "name": "string-operations"
    },
    {
        "new_path": "ci/benchmark-scripts/json-processing.rad",
        "old_path": null,
        "name": "json-processing"
    },
    {
        "new_path": "ci/benchmark-scripts/file-io.rad",
        "old_path": null,
        "name": "file-io"
    },
    {
        "new_path": "ci/benchmark-scripts/display-table.rad",
        "old_path": null,
        "name": "display-table"
    }
]

// ============================================================================
// PHASE 1: Build both binaries before running any benchmarks
// ============================================================================

// Build PR binary
print("ðŸ”¨ Building PR binary...")
$`make build`
pr_binary = "{pr_dir}/bin/radd"

// Clone and build base branch
print("ðŸ”„ Cloning and building base branch...")
temp_dir = "/tmp/rad-base-benchmark"
$`rm -rf {temp_dir}`
repo_url = "https://github.com/amterp/rad.git"
$`git clone --depth=1 --branch={base_ref} {repo_url} {temp_dir}`
$`cd {temp_dir} && make build`
base_binary = "{temp_dir}/bin/radd"

print("âœ… Both binaries built")
print("   PR binary: {pr_binary}")
print("   Base binary: {base_binary}")

// ============================================================================
// PHASE 2: Determine which benchmarks to run
// ============================================================================

// Find benchmarks that exist in PR
print("ðŸ”§ Checking for PR benchmarks...")
pr_benchmarks = []
for config in benchmark_configs:
    info = get_path(config.new_path)
    if info.exists:
        pr_benchmarks = pr_benchmarks + [config]
        print("âœ… Found PR benchmark: {config.new_path}")
    else:
        print("âš ï¸ Skipping missing PR benchmark: {config.new_path}")

if len(pr_benchmarks) == 0:
    print_err("âŒ No benchmark scripts found in PR")
    exit(1)

// Determine benchmark script paths for base (with fallback logic)
print("ðŸ”§ Resolving base benchmark paths...")
benchmark_pairs = []
for config in pr_benchmarks:
    pr_script = "{pr_dir}/{config.new_path}"

    // Try new path in base
    new_path = "{temp_dir}/{config.new_path}"
    if get_path(new_path).exists:
        benchmark_pairs = benchmark_pairs + [{
            "name": config.name,
            "pr_script": pr_script,
            "base_script": new_path
        }]
        print("âœ… {config.name}: using new path in base")
        continue

    // Try old path if available
    if config.old_path != null:
        old_path = "{temp_dir}/{config.old_path}"
        if get_path(old_path).exists:
            benchmark_pairs = benchmark_pairs + [{
                "name": config.name,
                "pr_script": pr_script,
                "base_script": old_path
            }]
            print("âœ… {config.name}: using old path in base")
            continue

    // Copy from PR if not in base (enables comparison for new benchmarks)
    print("ðŸ”§ {config.name}: not in base, copying from PR")
    target_path = "{temp_dir}/{config.new_path}"
    target_dir = join(split(target_path, "/")[:-1], "/")
    $`mkdir -p {target_dir}`
    $`cp {config.new_path} {target_path}`
    benchmark_pairs = benchmark_pairs + [{
        "name": config.name,
        "pr_script": pr_script,
        "base_script": target_path
    }]

if len(benchmark_pairs) == 0:
    print_err("âŒ No benchmarks available for comparison")
    exit(1)

print("ðŸ“Š Running {len(benchmark_pairs)} benchmarks")

// ============================================================================
// PHASE 3: Run interleaved benchmarks with hyperfine
// ============================================================================

// Build interleaved hyperfine command with -n labels
// Interleaving PR and Base runs for each benchmark ensures environmental noise
// affects both equally, improving statistical reliability
cmds = []
for pair in benchmark_pairs:
    // Use compound labels: "PR:name" and "Base:name"
    cmds = cmds + ["-n 'PR:{pair.name}' '{pr_binary} {pair.pr_script}'"]
    cmds = cmds + ["-n 'Base:{pair.name}' '{base_binary} {pair.base_script}'"]

cmd_str = join(cmds, " ")
results_file = "/tmp/benchmark-results.json"

print("ðŸƒ Running interleaved benchmarks (warmup=2, runs=5)...")
$`hyperfine {cmd_str} --warmup 2 --runs 5 --export-json {results_file}`

// ============================================================================
// PHASE 4: Parse results and compare
// ============================================================================

file_info = get_path(results_file)
if not file_info.exists:
    print_err("âŒ Benchmark results file not found: {results_file}")
    exit(1)

data = parse_json(read_file(results_file).content)
if type_of(data) == "error":
    print_err("âŒ Failed to parse benchmark data: {data}")
    exit(1)

// Build maps for PR and Base results using the -n labels
pr_results = {}
base_results = {}
for result in data.results:
    // result.command contains the label like "PR:for-loop-add"
    label = result.command
    parts = split(label, ":")
    if len(parts) != 2:
        print("âš ï¸ Unexpected label format: {label}")
        continue

    variant = parts[0]
    bench_name = parts[1]

    if variant == "PR":
        pr_results[bench_name] = result
    else if variant == "Base":
        base_results[bench_name] = result

// Compare results
comparisons = []
for bench_name in keys(pr_results):
    pr_result = pr_results[bench_name]
    base_result = base_results[bench_name]

    if base_result == null:
        print("âš ï¸ Skipping {bench_name} (no base result)")
        continue

    // Calculate performance change
    pr_mean = pr_result.mean * 1000  // Convert to milliseconds
    base_mean = base_result.mean * 1000
    diff_ms = pr_mean - base_mean
    diff_percent = base_mean > 0 ? round((diff_ms / base_mean) * 100, 2) : 0

    comparisons = comparisons + [{
        "name": bench_name,
        "base_mean_ms": round(base_mean, 2),
        "pr_mean_ms": round(pr_mean, 2),
        "diff_ms": round(diff_ms, 2),
        "diff_percent": diff_percent,
        "is_regression": diff_percent > 10  // Flag >10% slower as regression
    }]

// ============================================================================
// PHASE 5: Write results and cleanup
// ============================================================================

// Create result structure
result = {
    "comparisons": comparisons,
    "has_regressions": len([c for c in comparisons if c.is_regression]) > 0
}

// Write results for GitHub Actions
result_json = str(result)
write_result = write_file("/tmp/benchmark-comparison.json", result_json)
if type_of(write_result) == "error":
    print_err("âŒ Failed to write results: {write_result}")
    exit(1)

// Verify file was written
benchmark_file_info = get_path("/tmp/benchmark-comparison.json")
if benchmark_file_info.exists:
    print("âœ… Benchmark comparison file written successfully")
    print("ðŸ“ File size: {benchmark_file_info.size_bytes} bytes")
else:
    print_err("âŒ Benchmark comparison file was not created")
    exit(1)

// Clean up temporary directory
$`rm -rf {temp_dir}`

// Print summary
print("ðŸ“ˆ Benchmark comparison completed:")
for comparison in comparisons:
    change_icon = comparison.is_regression ? "ðŸ”º" : (comparison.diff_percent > 0 ? "ðŸ“ˆ" : "ðŸ“‰")
    print("  {comparison.name}:")
    print("    Base: {comparison.base_mean_ms}ms")
    print("    PR:   {comparison.pr_mean_ms}ms")
    print("    Change: {change_icon} {format_diff(comparison.diff_ms)}ms ({comparison.diff_percent}%)")

if result.has_regressions:
    print(yellow("âš ï¸  Performance regressions detected!"))

fn format_diff(diff_ms):
    sign = diff_ms >= 0 ? "+" : ""
    return "{sign}{abs(diff_ms)}"
